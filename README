# Superschedules Collector

Standalone Python tool for collecting event information from multiple sources
and posting it to the Superschedules API.

## Setup

```bash
pip install -r requirements.txt
```

## Running the Collector

Run all scrapers and the LLM agent and post results:

```bash
python -m jobs.run_everything
```

Environment variables are loaded from `.env` (see `.env.example`).

## Processing a Single URL

The collector can scrape event data from pages that expose JSON-LD metadata.
For example, to ingest events from the Needham Library events page:

```bash
python -m jobs.process_url https://needhamlibrary.org/events/
```

The script fetches the page, extracts any events, and submits them to the
Superschedules API.
