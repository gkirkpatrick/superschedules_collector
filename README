# Superschedules Collector

Standalone Python tool for collecting event information from multiple sources
and posting it to the Superschedules API.

## Setup

```bash
pip install -r requirements.txt
```

## Running the Collector

### Command Line Interface

Run all scrapers and the LLM agent and post results:

```bash
python -m jobs.run_everything
```

Process a single URL:

```bash
python -m jobs.process_url https://needhamlibrary.org/events/
```

### FastAPI Server

Start the API server for programmatic access:

```bash
# Development mode (auto-reload, debug logging)
python start_api.py

# Production mode 
python start_api.py --prod

# Custom port (default is 8001 to avoid conflict with main backend on 8000)
python start_api.py --port 8080
```

The API will be available at:
- **Base URL:** http://localhost:8001
- **Interactive docs:** http://localhost:8001/docs
- **Health check:** http://localhost:8001/health

### API Endpoints

- `GET /health` - Health check
- `GET /live` - Liveness probe for containers  
- `GET /ready` - Readiness probe for orchestration
- `POST /scrape` - Main endpoint for event scraping

Example API usage:

```bash
curl -X POST "http://localhost:8001/scrape" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://needhamlibrary.org/events/",
    "search_methods": ["jsonld", "llm"],
    "event_tags": ["workshop", "lecture"],
    "additional_info": {}
  }'
```

## Configuration

Environment variables are loaded from `.env` (see `.env.example`).
Supply your OpenAI key via `OPENAI_API_KEY`.

## How It Works

The collector uses a hierarchical fallback strategy:

1. **JSON-LD extraction** - First looks for structured Schema.org JSON-LD metadata
2. **LLM scraping** - Falls back to OpenAI's structured-output API for raw HTML parsing
3. **Pagination detection** - Automatically detects and handles paginated event listings

Set `SCRAPER_DEBUG=1` to log which scraper was used and the events returned.
