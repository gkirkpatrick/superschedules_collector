# LLM Testing Configuration
# This file configures which models to test and their settings

# Global test settings
global:
  output_dir: "results"
  timeout: 120  # seconds
  num_iterations: 1  # How many times to run each test
  
# Authentication configuration
auth:
  openai:
    type: "bearer"
    key_env: "OPENAI_API_KEY"
    key_file: "~/.secret_keys"
  anthropic:
    type: "bearer" 
    key_env: "ANTHROPIC_API_KEY"
    key_file: "~/.anthropic_key"
  ollama:
    type: "none"  # Local, no auth needed

# Model configurations to test
models:
  # OpenAI Models
  openai_gpt4o_mini:
    provider: "openai"
    name: "gpt-4o-mini"
    api_url: "https://api.openai.com/v1/chat/completions"
    auth: "openai"
    max_context: 128000
    test_contexts: [4000, 16000, 32000]  # Context sizes to test
    temperature: 0
    enabled: true
    
  # Anthropic Models  
  anthropic_sonnet:
    provider: "anthropic"
    name: "claude-3-5-sonnet-20241022"
    api_url: "https://api.anthropic.com/v1/messages"
    auth: "anthropic"
    max_context: 200000
    test_contexts: [4000, 16000, 50000, 100000]
    temperature: 0
    enabled: false  # Disable by default (requires API key)
    
  anthropic_haiku:
    provider: "anthropic"
    name: "claude-3-5-haiku-20241022"
    api_url: "https://api.anthropic.com/v1/messages"
    auth: "anthropic" 
    max_context: 200000
    test_contexts: [4000, 16000, 50000]
    temperature: 0
    enabled: false
    
  # Ollama Models (Local)
  ollama_gemma2_7b:
    provider: "ollama"
    name: "gemma2:latest"
    api_url: "http://localhost:11434/api/generate"
    auth: "ollama"
    max_context: 8192
    test_contexts: [2000, 4000, 8000]
    temperature: 0
    enabled: true
    
  ollama_gemma2_27b:
    provider: "ollama"
    name: "gemma2:27b"
    api_url: "http://localhost:11434/api/generate" 
    auth: "ollama"
    max_context: 8192
    test_contexts: [2000, 4000, 8000]
    temperature: 0
    enabled: false  # Requires manual pull
    
  ollama_llama32_3b:
    provider: "ollama"
    name: "llama3.2:3b"
    api_url: "http://localhost:11434/api/generate"
    auth: "ollama"
    max_context: 131072
    test_contexts: [4000, 16000, 32000]
    temperature: 0
    enabled: false
    
  ollama_qwen25_7b:
    provider: "ollama"
    name: "qwen2.5:7b"
    api_url: "http://localhost:11434/api/generate"
    auth: "ollama"
    max_context: 32768
    test_contexts: [4000, 16000, 32000]
    temperature: 0
    enabled: false

# Test data configuration
test_data:
  prompts_file: "parsed_prompts.json"
  schema_validation: true
  json_equivalence_check: true
  
# Reporting configuration
reporting:
  save_individual_responses: true
  save_summary_stats: true  
  comparison_matrix: true
  performance_charts: false  # Requires matplotlib
  
# Advanced settings
advanced:
  parallel_requests: false  # Run models in parallel (be careful with rate limits)
  retry_failed: true
  retry_count: 2
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR